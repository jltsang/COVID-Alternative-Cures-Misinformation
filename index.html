<!DOCTYPE HTML>
<!--
	Miniport by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>
	<head>
		<title>PH Twitter Fake News Analysis</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="assets/css/main.css" />
		<link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/11.7.0/styles/hybrid.min.css">
	</head>
	<body class="is-preload">

		<!-- Nav -->
			<nav id="nav">
				<ul class="container">
					<li><a href="#top">Top</a></li>
					<li><a href="#overview">Overview</a></li>
					<li><a href="#data">Data</a></li>
					<li><a href="#methods">Methods</a></li>
					<li><a href="#results">Results</a></li>
					<li><a href="#team">Team</a></li>
				</ul>
			</nav>

		<!-- Home -->
			<article id="top" class="wrapper style1">
				<div class="container">
					<div class="row">
						<div class="col-4 col-5-large col-12-medium">
							<span class="image fit"><img src="images/logo_1.png" alt="" /></span>
						</div>
						<div class="col-8 col-7-large col-12-medium">
							<header>
								<h1>Hi. We are <strong>DATA ATLAS</strong>.</h1>
							</header>
							<p>We are conducting an analysis on tweets from 2016-2022 to determine events that may have triggered the spread of mis/disinformation regarding tuob and herbal medicines' ability to cure COVID-19.</em> We aim to process and investigate the data from these tweets to hopefully gain more insight about the proliferation of misinformation on Twitter during the pandemic.</p>
							<p>Data Science Team</p>
							<ul style="list-style-type:none; margin-top:-30px;">
							  <li>Francis Mamuyac Albarracin, WFU</li>
							  <li>Zachary R. Nabong, WFW</li>
							  <li>Jeremy King L. Tsang, WFW</li>
							</ul>
						</div>
					</div>
				</div>
			</article>

		<!-- Overview -->
			<article id="overview" class="wrapper style2">
				<div class="container">
					<header>
						<h2>Why are we doing this?</h2>
					</header>
					<div class="row aln-center">
						<div class="col-12">
							<section class="box style1">
								<h3>Motivation</h3>
								<p>The circulation of mis/disinformation on COVID-19 has been shown to increase public anxiety and distrust (Greene & Murphy, 2021). The warp in public perception and public distrust can lead to serious repercussions given the circumstances, as sick individuals may delay seeking proper medical treatment due to their false belief on alternative medicines, thus putting their life in greater danger.</p>
							</section>
						</div>	
						<div class="col-4 col-6-medium col-12-small">
							<section class="box style1">
								<span class="icon featured fa-comments"></span>
								<h3>Research Question</h3>
								<p>What events have triggered the spread of mis/disinformation about tuob and herbal medicines being able to cure COVID-19?</p>
							</section>
						</div>
						<div class="col-4 col-6-medium col-12-small">
							<section class="box style1">
								<span class="icon solid featured fa-bullseye"></span>
								<h3>Hypothesis</h3>
								<p>The timing of the endorsements of famous personalities correlates to the spread of mis/disinformation on alternative cures for COVID-19.</p>
							</section>
						</div>
						<div class="col-4 col-6-medium col-12-small">
							<section class="box style1">
								<span class="icon featured fa-times-circle"></span>
								<h3>Null Hypothesis</h3>
								<p>The timing of the endorsements of famous personalities has no correlation to the spread of mis/disinformation on alternative cures for COVID-19.</p>
							</section>
						</div>
						<div class="col-12">
							<section class="box style1">
								<h3>Action Plan</h3>
								<p>Analyze the frequency and posting time of mis/disinformation tweets about tuob and herbal medicines being able to cure COVID-19.</p>
							</section>
						</div>
					</div>
			</article>

		<!-- Data -->
			<article id="data" class="wrapper style1">
				<div class="container">
					<header>
						<h2>We mined Twitter for fake news data on COVID-19 alternative remedies.</h2>
					</header>
					<div class="row">
						<div class="col-4 col-6-medium col-12-small">
							<article class="box style2">
								<a href="#" class="image featured"><img src="images/pic01.jpg" alt="" /></a>
								<h3><a href="#">Topic</a></h3>
								<p>Ornare nulla proin odio consequat.</p>
							</article>
						</div>
						<div class="col-4 col-6-medium col-12-small">
							<article class="box style2">
								<a href="#" class="image featured"><img src="images/pic02.jpg" alt="" /></a>
								<h3><a href="#">Keywords</a></h3>
								<p>Ornare nulla proin odio consequat..</p>
							</article>
						</div>
						<div class="col-4 col-6-medium col-12-small">
							<article class="box style2">
								<a href="#" class="image featured"><img src="images/pic03.jpg" alt="" /></a>
								<h3><a href="#">Tools</a></h3>
								<p>Ornare nulla proin odio consequat.</p>
							</article>
						</div>
					</div>
					<footer>
						<p>Curious of our data?</p>
						<a href="https://docs.google.com/spreadsheets/d/1soZQfU-6A4gmQRhCd_UsMKVMqVqoaEwg0zM01--WByw/edit#gid=107810933" class="button large scrolly" target="_blank">Take a look at it here!</a>
					</footer>
				</div>
			</article>

		<!-- Data exploration -->
			<article id="methods" class="wrapper style2">
				<div class="container">
					<header>
						<h2>Here is how we explored the data,</h2>
					</header>
					<div class="container" id="preprocessing" style="text-align:left; margin-left: 40px;">
						<h3>1. Preprocessing</h3>
						<p>Before we begin exploring the data, preprocessing techniques must first be applied in order to ensure the cleanliness of the data.</p>
						<div class="container" style="text-align:left;">
							<h3>1.a. Handling missing values</h3>
							<p>When looking for missing values, we can observe that `Account bio`, `Location`, `Tweet Translated`, `Screenshot`, `Views`, `Rating`, `Remarks`, `Reviewer`, `Review` are all listed.</p>
							<pre style="tab-size: 4;">
								<code class="language-python">print("Columns with missing/nan values:")
print("Data set:", df_dataset.columns[df_dataset.isna().any()].tolist())</code>
							</pre>
							<figure>
								<img src="1a-output1.png" alt="" srcset="" style="width: 100%">
								<figcaption>Codeblock output</figcaption>
							</figure>
							<p>Some of these columns, including `Tweet Translated`, `Screenshot`, `Views`, `Rating`, `Remarks`, `Reviewer`, and `Review`, are optional columns that we have opted not to fill out during data collection.</p>
							<p>On the other hand, `Account bio` and `Location` are columns with missing values given that Twitter users may opt to leave these fields blank. Given that there is already a limited amount of data, dropping the samples is not ideal. In addition, `Account bio` and `Location` are not relevant variables for what we want to analyse later on. Therefore, given the irrelevancy of said columns and small number of samples, we instead choose to drop the columns instead of the samples.</p>
							<pre style="tab-size: 4;">
								<code class="language-python">df_dataset = df_dataset.drop(['Account bio', 'Location', 'Tweet Translated', 'Screenshot', 'Rating', 'Remarks','Reviewer', 'Review', 'Views'], axis=1)	</code>
							</pre>
							<p>In addition to columns with missing values, we can also look to drop columns that are completely filled up but are irrelevant to analysis. These include `ID`, `Timestamp`, `Tweet URL`, `Group`, `Collector`, `Category`, `Topic`, `Keywords`, `Account handle`, `Account name`, `Joined`, `Tweet Type`, and `Reasoning`.</p>
							<pre style="tab-size: 4;">
								<code class="language-python">df_dataset = df_dataset.drop(['ID', 'Timestamp', 'Group', 'Collector', 'Category', 'Topic', 'Keywords', 'Reasoning', 'Tweet URL', 'Account handle', 'Account name', 'Joined', 'Tweet Type'], axis=1)	</code>
							</pre>
							<p>With that, we are left with a much more compact dataset with only 10 columns, while maintaining a total of 150 samples.</p>
							<pre style="tab-size: 4;">
								<code class="language-python">df_dataset.head(10).style.set_properties(**{'text-align': 'left'})	</code>
							</pre>
							<figure>
								<img src="1a-output2.png" alt="" srcset="" style="width: 100%">
								<figcaption>The first 10 rows of the dataframe</figcaption>
							</figure>
						</div>
						<div class="container">
							<h3>1.b. Handling outliers</h3>
							<p>After handling missing values, we turn our attention into possible outlier values. While we were able to preserve our samples by dropping columns when handling missing values, the same cannot be done for outlier data. These data can heavily affect our analysis and results later on.</p>
							<p>Using the following code, we begin by looking into our outlier data based on each variable.</p>
							<pre style="tab-size: 4;">
								<code class="language-python">from sklearn.preprocessing import StandardScaler
import seaborn as sns

# Standardize the features to get z-scores
df_scaled = df_dataset.copy(deep=True)
scaler = StandardScaler()
df_scaled[['Followers']] = scaler.fit_transform(df_scaled[['Followers']])

followers_zscore = df_scaled['Followers']
n_out1 = len(followers_zscore[abs(followers_zscore) > 1])
n_out2 = len(followers_zscore[abs(followers_zscore) > 2])
n_out3 = len(followers_zscore[abs(followers_zscore) > 3])
print(f"Number of outliers in 'Followers' (std=1): {n_out1}")
print(f"Number of outliers in 'Followers' (std=2): {n_out2}")
print(f"Number of outliers in 'Followers' (std=3): {n_out3}")

# Define function for coloring bars
def color_bars(ax, df):
	for i in range(len(df)):
		val = abs(df.loc[i, 'Followers'])
		if val <= 1:
			color = 'b'
		elif val <= 2:
			color = 'g'
		elif val <= 3:
			color = 'y'
		else:
			color = 'r'
		ax.get_children()[i].set_color(color)

# Create a figure with two subplots
fig, (ax1, ax2) = plt.subplots(nrows=2, figsize=(16, 12))

# Plot the Followers distribution
sns.barplot(x=df_scaled.index, y=df_scaled['Followers'], ax=ax1)

sns.despine(ax=ax1)
ax1.set(title='Followers Outliers', xlabel='Sample', ylabel='Followers (z-score)')
ax1.set_xticks([0, len(df_scaled)-1])
ax1.axhline(y=3, xmin=-0.5, xmax=len(df_scaled)-0.5, color='red', alpha=0.5, ls='--') # Standard deviation lines
ax1.axhline(y=-3, xmin=-0.5, xmax=len(df_scaled)-0.5, color='red', alpha=0.5, ls='--')
color_bars(ax1, df_scaled) # Color the bars

# Plot the Followers distribution, but sorted
df_scaled_sorted = df_scaled.sort_values(['Followers']).reset_index(drop=True)
sns.barplot(x=df_scaled_sorted.index, y=df_scaled_sorted['Followers'], ax=ax2)

sns.despine(ax=ax2)
ax2.set(title='Followers Outliers', xlabel='Sample (sorted)', ylabel='Followers (z-score)')
ax2.set_xticks([0, len(df_scaled_sorted)-1])
ax2.axhline(y=3, xmin=-0.5, xmax=len(df_scaled_sorted)-0.5, color='red', alpha=0.5, ls='--') # Standard deviation lines
ax2.axhline(y=-3, xmin=-0.5, xmax=len(df_scaled_sorted)-0.5, color='red', alpha=0.5, ls='--')
color_bars(ax2, df_scaled_sorted) # Color the bars

plt.tight_layout()
plt.show()</code>
							</pre>
							<figure>
								<img src="1b-output1.png" alt="" style="width: 100%">
								<img src="1b-output2.png" alt="" style="width: 100%">
								<img src="1b-output3.png" alt="" style="width: 100%">
								<img src="1b-output4.png" alt="" style="width: 100%">
								<figcaption>Followers, Likes, Replies, and Retweets distributions</figcaption>
							</figure>
							<p>We can observe that outlier data do exist in our dataset. Even more concerning is the fact that these outlier data go as high 10 standard deviations away (std=10). In order to make sure we that these samples would not skew our analysis later, we can only drop them.</p>
							<pre style="tab-size: 4;">
								<code class="language-python">def remove_outliers(df, feature, threshold):
	df_scaled = df.copy(deep=True)
	scaler = StandardScaler()
	df_scaled[[feature]] = scaler.fit_transform(df_scaled[[feature]])
	mask = abs(df_scaled[feature]) <= threshold
	return df[mask].reset_index(drop=True)

threshold = 3
for col in df_dataset.select_dtypes(include='number').columns:
	df_dataset = remove_outliers(df_dataset, col, threshold)</code>
							</pre>
						</div>
						<div class="container">
							<h3>1.c. Ensuring format consistency</h3>
							<p>In order to ensure the validity and consistency of the data, a quick set of tests are conducted to find incorrectly-formatted data. To accomplish this, we created a mask with the correct format for each column and compared it with the dataset entries. As seen below we can see that the results show no formatting mistakes for all columns.</p>
							<pre style="tab-size: 4;">
								<code class="language-python">import re

print("Checking # of incorrectly-formatted rows...")
# Check account type
account_type_mask = df_dataset['Account type'].isin(['Anonymous','Identified','Media'])
print(f"For column \"Account type\": {df_dataset[~account_type_mask].shape[0]}")

# Check date posted (DD/MM/YY HH:MM)
date_posted_pattern = re.compile(r'(0[1-9]|[1-2][0-9]|3[0-1])\/(0[1-9]|1[0-2])\/([0-9]{2}) (2[0-3]|[01]?[0-9]):[0-5][0-9]')
date_posted_mask = df_dataset['Date posted'].astype(str).str.match(date_posted_pattern)
print(f"For column \"Date posted\": {df_dataset[~date_posted_mask].shape[0]}")

# Check content type
content_type_mask = df_dataset['Content type'].isin(['Rational','Emotional','Transactional'])
print(f"For column \"Content type\": {df_dataset[~content_type_mask].shape[0]}")

# Check for valid whole numbers
whole_number_pattern = re.compile(r'^\d+$')
for col in ['Following', 'Followers', 'Likes', 'Replies', 'Retweets', 'Quote Tweets']:
	number_mask = df_dataset[col].astype(str).str.match(whole_number_pattern)
	print(f"For column \"{col}\": {df_dataset[~number_mask].shape[0]}")</code>
							</pre>
							<figure style="text-align: center;">
								<img src="1c-output1.png" alt="" style="max-width: 100%;">
								<figcaption>Output of the codeblock</figcaption>
							</figure>
						</div>
						<div class="container">
							<h3>1.d. Categorical data encoding</h3>
							<p>Looking at our remaining columns, we can observe that there are two categorical variables, `Account type` and `Content type`. In order to use the following variable later on, we will need to convert these columns to numerical data. To accomplish this, we perform one-hot encoding on both categories.</p>
							<pre style="tab-size: 4;">
								<code class="language-python"># One-hot encode Account Type column
account_type_dummies = pd.get_dummies(df_dataset['Account type'], prefix='Account Type')
df_dataset = pd.concat([df_dataset, account_type_dummies], axis=1)

# One-hot encode Content Type column
content_type_dummies = pd.get_dummies(df_dataset['Content type'], prefix='Content Type')
df_dataset = pd.concat([df_dataset, content_type_dummies], axis=1)

print(df_dataset.columns)</code>
							</pre>
							<figure style="text-align: center;">
								<img src="1d-output1.png" alt=""  style="max-width: 100%;">
								<figcaption>Columns of the dataframe</figcaption>
							</figure>
							<p>This will make it easier to perform analysis and modeling with these variables later on, espcially beside other numerical data.</p>
						</div>
						<div class="container">
							<h3>1.e. Natural Language Processing</h3>
							<p>Now that we have made sure that our data set has no blemishes, we can begin with the processing. We make use of the NLP (Natural Language Processing) module found within Python in order for us to turn our dataset to a more structured and workable set</p>
							<p>We begin our process by 'cleaning' the dataset and transforming special symbols such as emojis into words, as the usage of emojis may provide extra context to the tweets they are being used in.</p>
							<pre style="tab-size: 4;">
								<code class="language-python">import pandas as pd
import re
import copy
import nltk

# Handle Emojis [2]
url_emoji = "https://drive.google.com/uc?id=1G1vIkkbqPBYPKHcQ8qy0G2zkoab2Qv4v"
df_emoji = pd.read_pickle(url_emoji)
df_emoji = {v: k for k, v in df_emoji.items()}

def emoji_to_word(text):
	for emot in df_emoji:
	text = re.sub(r'('+emot+')', "_".join(df_emoji[emot].replace(",","").replace(":","").split()), text)
	return text

# Handle Emoticons [2]
url_emote = "https://drive.google.com/uc?id=1HDpafp97gCl9xZTQWMgP2kKK_NuhENlE"
df_emote = pd.read_pickle(url_emote)

def emote_to_word(text):
	for emot in df_emote:
		text = re.sub(u'('+emot+')', "_".join(df_emote[emot].replace(",","").split()), text)
		text = text.replace("<3", "heart" ) # not included in emoticons database
	return text

texts = copy.deepcopy(df_dataset['Tweet'])

texts = [emoji_to_word(t) for t in texts]
texts = [emote_to_word(t) for t in texts]</code>
							</pre>
							<p>after converting the emojis, we made use of a new array called `texts` which we used to store the new tweets as we wish to turn all the words into lowercase in order to make the dataset easier to work with.</p>
							<pre style="tab-size: 4;">
								<code class="language-python">import string

# convert to lowercase
texts = [t.lower() for t in texts]

# replace tuob and suob with steam
texts = [t.replace('tuob', 'steam').replace('suob', 'steam') for t in texts]

# remove punctuation
texts = [t.translate(str.maketrans('', '', string.punctuation)) for t in texts]</code>
							</pre>
							<p>You may have noticed that we included a line of code that replaced `tuob` or `suob` with `steam`. This was done by us as a sort of bandaid solution as the translator was either ignoring the word entirely or misreading the word as `subo` leading to incorrect translation.</p>
							<p>In order to maximize the use of the NLP module, we decided to translate the tweets first from Tagalog to English as the module was not optimized to handle tweets in Tagalog.</p>
							<pre style="tab-size: 4;">
								<code class="language-python"># Removing stopwords might be tedious for multilingual texts
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize

# CHEAP SOLUTION: translate texts to English (this is not 100% accurate)
from googletrans import Translator

# translate to English
translator = Translator()
texts_en = [t.text for t in translator.translate(texts, src='tl', dest='en')]</code>
							</pre>
							<p>Now that we converted the dataset into English, we could know start with actually structuring the data. First we started by tokenizing the data. Tokenizing is when we conviently split up text word by word. This allowed us to work with smaller pieces of data that were still meaningful even outside of context of the rest of the text. </p>
							<p>After tokenizing, we also made sure to remove the stopwords in our dataset. Stopwords are pretty much the words we don't really want to care about, so we filter them out of our data frame.</p>
							<pre style="tab-size: 4;">
								<code class="language-python">texts_tok = []
for text in texts_en:
	# tokenize the text into words
	words = word_tokenize(text)

	# remove stopwords
	filtered_words = [word for word in words if word.lower() not in stopwords.words('english')]

	# convert back into sentence
	filtered_sentence = ' '.join(filtered_words)
	texts_tok.append(filtered_sentence)</code>
							</pre>
							<p>The process of tokenizing and stopwords removal was similar to our earlier methods, as you can see with the use of a new array called `texts_tok`.</p>
							<p>With the data now easier to work with, we simplified it further with Stemming and Lemmatization. Stemming is when you strip a word down to its root word. Kind of like turning a fraction into its simplest form. Lemmatization is like stemming but will give you a complete word that makes sense when you read it instead of a fragment of a word.</p>
							<p>We accomplished stemming and lemmatization by making use of separate variables called `stemmer` and `lemmatizer` respectively, which was used in the `stem_lem` function for the process of stemming/lemmatization. </p>
							<pre style="tab-size: 4;">
								<code class="language-python">from nltk.stem import PorterStemmer, WordNetLemmatizer

# Initialize the stemmer and lemmatizer
stemmer = PorterStemmer()
lemmatizer = WordNetLemmatizer()

texts_stem, texts_lem = [], []

def stem_lem(text):
	words = text.split()

	# Stem each word
	stemmed_words = [stemmer.stem(word) for word in words]
	
	# Lemmatize each word
	lemmatized_words = [lemmatizer.lemmatize(word) for word in words]
	
	# Return the stemmed and lemmatized words as a tuple
	texts_stem.append(stemmed_words)
	texts_lem.append(lemmatized_words)

	return (stemmed_words, lemmatized_words)


# Process each text in the array
processed_texts = [stem_lem(t) for t in texts_tok]


df_sl = pd.DataFrame({'Original': df_filt['Tokenized'], 'Stemmed': texts_stem, 'Lemmatized': texts_lem})
df_sl.style.set_properties(**{'text-align': 'left'})</code>
							</pre>
							<figure>
								<img src="1e-output1.png" alt="" style="width:100%;">
								<figcaption>First 10 rows of the new dataframes</figcaption>
							</figure>
							<p>The final result from all the processing was the creation of a final dataframe called `df_sl` and this dataframe was used in getting the common `stemmed` and `lemmatized` words that gave us an insight as to which "cure" was most prevalent among the tweets.</p>
							<pre style="tab-size: 4;">
								<code class="language-python">stemm_freq = df_sl['Stemmed'].explode().value_counts()
stemm_freq[:30].to_frame().style</code>
							</pre>
							<figure style="text-align: center;">
								<img src="1e-output2.png" alt="">
								<figcaption>Most common words from the Stemmed dataframe</figcaption>
							</figure>
							<pre style="tab-size: 4;">
								<code class="language-python">lemm_freq = df_sl['Lemmatized'].explode().value_counts()
lemm_freq[:30].to_frame().style</code>
							</pre>
							<figure style="text-align: center;">
								<img src="1e-output3.png" alt="">
								<figcaption>Most common words from the Lemmatized dataframe</figcaption>
							</figure>
							<p>From the results, it was concluded that `steam` in this case `tuob` or `suob` was the most talked about "cure" for COVID, followed by `ginger`, `lemon`, `salt` and lastly, `vitamins`.</p>
						</div>
						<div class="container">
							<h3>1.f. Time series analysis binning</h3>
							<p>In order to do a time series analysis of our data, specifically the `Date posted` column, there is the option to either interpolate or bin the time points. However given the large range of time in which the data points are collected (2020-2022) and the small size of the dataset, binning is a much more feasible method. 14-day binning and monthly binning were chosen as the intervals. Given the limited dataset, monthly binning has a higher change of providing a clear distinction of spikes and peaks, while 14-day binning would make identifying which specific events (if there are any) may have resulted in the spike/peak. The process of binning can be found with the visualization of the time series thorough line plots.</p>
						</div>
					</div>
					<div class="container" id="visualization" style="text-align:left; margin-left: 40px;">
						<h3>2. Visualization</h3>
						<p>Now we can explore our data by visualizing its features.</p>
						<div class="container">
							<h3>2.a. Bar Plot</h3>
							<p>The first thing we are interested in looking into is finding possible connections between `Account Type` and `Content Type`. To answer this question, we create a Grouped Barplot that shows the number of samples for each `Content Type` grouped by `Account Type`.</p>
							<pre style="tab-size: 4;">
								<code class="language-python"># Group the DataFrame by 'Account type' and 'Content type' and count the number of occurrences
df_count = df_dataset.groupby(['Account type', 'Content type'])['Content type'].count().reset_index(name='Count')

# Create a nested bar chart with Plotly
fig = px.bar(df_count, x="Account type", y="Count", color="Content type", barmode="group")

# Customize the layout of the chart
fig.update_layout(
	title="Number of Samples per Content Type by Account Type",
	xaxis_title="Account Type",
	yaxis_title="Count",
	legend_title="Content Type",
	font=dict(
		family="Arial",
		size=14,
		color="#333333"
	)
)

# Show the chart
fig.show()</code>
							</pre>
							<figure>
								<embed type="text/html" src="bar.html" style="width: 90%; height: 500px;"> 
								<figcaption style="margin-top: -30px;">Bar graph visualization</figcaption>
							</figure>
							<p>The resulting graph shows some interesting results. The most obvious result is that `Emotional` tweets far outnumber `Rational`. `Transactional` tweets can also be seen to be very rare, with only one instance present. Additionally, most mis/disinformation comes from `Anonymous` account types, followed by `Identified`, and finally `Media` with one instance.</p>
							<p>A more notable observation is the distribution and ratio of `Emotional` and `Rational` tweets for `Anonymous` and `Identified`. `Identified` accounts can be seen to have close to a 2:1 split for `Emotional` and `Rational` tweets, however `Anonymous` tweets have a significantly higher ratio close to 5:1. This uneven ratio may suggest that `Anonymous` accounts are more likely to post mis/disinformation tweets `Emotionally`.</p>
						</div>
						<div class="container">
							<h3>2.b. Scatterplots</h3>
							<p>Next, we want to look into the distribution of numerical features such as `Following`, `Followers`, `Likes`, `Replies`, `Retweets`, `Quote Tweets` among `Account Types` using a Scatter Plot Matrix plot to find possible relationships between them.</p>
							<pre style="tab-size: 4;">
								<code class="language-python"># Define the categorical and numerical features
feat_cat = ['Account Type_Anonymous', 'Account Type_Identified', 'Account Type_Media', 'Content Type_Emotional', 'Content Type_Rational', 'Content Type_Transactional']
feat_num = ['Following', 'Followers', 'Likes', 'Replies', 'Retweets', 'Quote Tweets']</code>
							</pre>
							<pre style="tab-size: 4;">
								<code class="language-python">import matplotlib.pyplot as plt
import seaborn as sns

# Set plot styling
sns.set(rc={'figure.figsize':(16,9)})
sns.set_theme(style="darkgrid")

# Plot the distribution of numerical features
g = sns.pairplot(data=df_dataset[feat_num+['Account type']], hue='Account type')
g.fig.suptitle("Twitter Data - Numerical Features", fontsize=18, y=1.05) # y: vertical position of title
plt.show()</code>
							</pre>
							<figure>
								<img src="2b-output1.png" alt="" style="width: 100%;">
								<figcaption>Scatterplot visualization</figcaption>
							</figure>
							<p>Looking in the resulting plot, there no standout patterns can be observe. `Anonymous` and `Identified` `Account Types` are scattered randomly. Additionally, `Media` `Account Types` may seem to be missing in the scatter plots due to only having one sample in the entirety of the dataset, as seen in the bar plot.</p>
						</div>
						<div class="container">
							<h3>2.c. Heat Map</h3>
							<p>We also want to check if there are any correlations between the numerical features. We use a heat map in order to visualize possible correlations between features.</p>
							<pre style="tab-size: 4;">
								<code class="language-python"># Correlation plot (heat map)
import plotly.express as px

num_features = ['Following','Followers', 'Likes', 'Replies', 'Retweets']
corr = df_dataset[num_features].corr() # Use Spearman for non-linear corr: (method='spearman')

fig = px.imshow(corr,
		color_continuous_scale='RdBu',
		zmin=-1,
		zmax=1,
		labels=dict(x='Features', y='Features', color='Correlation'),
		x=corr.columns,
		y=corr.columns)
fig.show()</code>
							</pre>
							<figure style="text-align: center;">
								<embed type="text/html" src="heatmap.html" style="width: 600px; max-width: 100%; height: 500px;">
								<figcaption style="margin-top: -3em;">Heatmap visualization</figcaption>
							</figure>
							<p>Looking at the resulting heat map, we can see that there is little correlation between the numerical features. The numerical features with the greatest correlation are `Following` and `Followers`, which may be due to the fact that the more you follow other users, the more likely that others may follow back, cause the said relation. Aside from that, there are no other strong correlations that can be observed.</p>
						</div>
						<div class="container">
							<h3>2.d. Line Plot (Time-series)</h3>
							<p>The last thing we want to look into are possible trends are present in mis/disinformation. To do this, we create two line plots, one for 14 day binning and another for monthly binning.</p>
							<pre style="tab-size: 4;">
								<code class="language-python">df_dataset['Date posted'] = pd.to_datetime(df_dataset['Date posted'])
df_dataset_14day = df_dataset.groupby(pd.Grouper(key='Date posted', freq='14D',convention='start')).size()

# Convert the pandas series to a DataFrame
df_dataset_14day = df_dataset_14day.to_frame(name='14 Day Tweet Count')

# Reset the index and rename the column to 'Date posted'
df_dataset_14day = df_dataset_14day.reset_index().rename(columns={'Date posted': 'Date posted', 0: '14 Day Tweet Count'})

# Create a line plot using Plotly
fig = px.line(df_dataset_14day, x='Date posted', y='14 Day Tweet Count', title='14 Day Binning Tweet Count')
fig.show()</code>
							</pre>
							<figure>
								<embed type="text/html" src="line-14day.html" style="width: 100%; height:500px;"> 
								<figcaption style="margin-top: -3em;">Line plot visualization (14 day binning)</figcaption>
							</figure>
							<pre style="tab-size: 4;">
								<code class="language-python">df_dataset['Date posted'] = pd.to_datetime(df_dataset['Date posted'])
df_dataset_monthly = df_dataset.groupby(pd.Grouper(key='Date posted', freq='1M',convention='start')).size()

# Convert the pandas series to a DataFrame
df_dataset_monthly = df_dataset_monthly.to_frame(name='Monthly Tweet Count')

# Reset the index and rename the column to 'Date posted'
df_dataset_monthly = df_dataset_monthly.reset_index().rename(columns={'Date posted': 'Date posted', 0: 'Monthly Tweet Count'})

# Create a line plot using Plotly
fig = px.line(df_dataset_monthly, x='Date posted', y='Monthly Tweet Count', title='Monthly Binning Tweet Count')
fig.show()</code>
							</pre>
							<figure>
								<embed type="text/html" src="line-1month.html" style="width: 100%; height: 500px;"> 
								<figcaption style="margin-top: -3em;">Line plot visualization (Monthly binning)</figcaption>
							</figure>
							<p>Looking at our time series data, we can quickly observe that there are (3) major peaks in both the 14 day binning and monthly binning. Most notably, when we compare the monthly binning side-by-side to the number of active cases of COVID-19, we can observe that the spikes happen at around the same time.</p>
							<figure style="text-align: center;">
								<img src="2d-output1.png" alt="" style="max-width: 100%;">
								<figcaption>Active COVID cases line graph (Philippines)</figcaption>
							</figure>
						</div>
					</div>
					<footer>
						<p>Want a more detailed view of our data exploration?</p>
						<a href="https://colab.research.google.com/drive/1tcgCDNkg-jZN13Bc0dKArLcq4YV9d928?usp=sharing" class="button large scrolly" target="_blank">Click here!</a>
					</footer>
				</div>
			</article>

		<!-- Results -->
		
			<article id="results" class="wrapper style1">
				<div class="container">
					<header>
						<h2>Here's what we found out.</h2>
					</header>
					<div class="container" id="statistical-modeling" style="text-align:left; margin-left: 40px;">
						<h3>1. Statistical modeling</h3>
						<p>
							Before perfoming any statistical tests, we will first be identifying the dates of key endorsements for both tuob or suob and salabat. 
							The endorsement of tuob/suob that become prominent came from the ```Cebu Governor Gwendolyn Garcia```. 
							This endorsement started as early as June 1, 2020, with a fact check article by Rappler releasing at June 30, 2020 
							(<a href="https://www.rappler.com/newsbreak/fact-check/265273-tuob-cure-covid-19/">Rappler</a>), and VERA Files on July 2, 2020 
							(<a href="https://verafiles.org/articles/vera-files-fact-sheet-what-tuob-or-steam-inhalation-therapy">VeraFiles</a>). 
						</p>
						<p>
							On the other hand, the misinformation on salabat as a cure for COVID-19 can be most commonly traced to popular facebook groups such as ```Natural Cures : Halamang Gamot```, 
							```Natural Herbs Halamang Gamot atbp Lunas```, and ```Our Daily Health```. These groups, which had ten thousands to hundred thousands of followers posted the misinformation 
							around February 7-9, 2020. The fact checking articles for these were released February 12, 2020 (<a href="https://www.rappler.com/newsbreak/fact-check/251567-natural-ginger-ale-cure-coronavirus/">Rappler</a>) 
							and February 18, 2020 (<a href="https://verafiles.org/articles/vera-files-fact-check-fb-posts-claiming-2019-ncov-only-survi">VERAFiles</a>).
						</p>
						<p>
							Now that we have identified the timing of endorsements, we can create segments of data for before and after the event and fit ```regression models``` on the segments. 
							With the regression models for both before and after the event, we can compare their slopes using ```T-Test``` in order to identify if there is significant difference between the two time periods.
						</p>
						<div class="container" style="text-align:left;">
							<h3>1.a. Tuob/Suob</h3>
							<div class="container"  style="text-align:left;">
								<h4>Segmented Linear Regression</h4>
								<p>
									Starting with tuob/suob, we get 6 points of time before and after the endorsement, which means that we are observing a 6 month time period 
									(3 months prior and 3 months post). After we have obtained both segments, we perform linear regression on both.
								</p>
								<pre style="tab-size: 4;"><code class="language-python">## Perform regression modeling
from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score, mean_squared_error

import statsmodels.api as sm

# Convert datetime to int
x = df_dataset_count['date'].astype(int) / 10**9  # Convert to seconds (UNIX epoch start)
x = x.values.reshape(-1, 1)

y = df_dataset_count['count']

# Divide the dataset based on the date (before and after June 1, 2020)
before_mask = df_dataset_count['date'] < pd.Timestamp('2020-06-01')
after_mask = df_dataset_count['date'] >= pd.Timestamp('2020-06-01')

x_before = x[before_mask][-6:]
y_before = y[before_mask][-6:]

x_after = x[after_mask][:6]
y_after = y[after_mask][:6]</code></pre>
								<pre style="tab-size: 4;"><code class="language-python">#--------------------------------------------------------------
# Linear Regression Model - Before Edorsement
#--------------------------------------------------------------

# Stastical approach
x_lms = sm.add_constant(x_before)
linear_model_stat = sm.OLS(y_before, x_lms)
lms_results = linear_model_stat.fit()
p_values = lms_results.pvalues[1:]

# Calculate predicted values using the statistical slope
y_linear_before = x_lms.dot(lms_results.params)

# Calculate R2 and RMSE for linear regression model
linear_r2 = r2_score(y_before, y_linear_before)
linear_rmse = np.sqrt(mean_squared_error(y_before, y_linear_before))

# Get the slope (coefficient) from the fitted model
slope_before = lms_results.params[1]
print("Slope of the line (Statistical Approach):", slope_before)

print("\nModel Evaluation")
print("Linear Regression: RMSE=%.2f, R2=%.2f" % (linear_rmse, linear_r2))
for i, p_value in enumerate(p_values.index):
  print(f'P({p_value}): {p_values[i]}')

if any(p_values <= 0.05):
  print("There is a significant relationship between the predictor and the response\n")
else:
  print("There is no significant relationship between the predictor and the response\n")

#--------------------------------------------------------------
# Linear Regression Model - After Edorsement
#--------------------------------------------------------------
# Stastical approach
x_lms = sm.add_constant(x_after)
linear_model_stat = sm.OLS(y_after, x_lms)
lms_results = linear_model_stat.fit()
p_values = lms_results.pvalues[1:]

# Calculate predicted values using the statistical slope
y_linear_after = x_lms.dot(lms_results.params)

# Calculate R2 and RMSE for linear regression model
linear_r2 = r2_score(y_after, y_linear_after)
linear_rmse = np.sqrt(mean_squared_error(y_after, y_linear_after))

# Get the slope (coefficient) from the fitted model
slope_after = lms_results.params[1]
print("Slope of the line (Statistical Approach):", slope_after)

print("\nModel Evaluation")
print("Linear Regression: RMSE=%.2f, R2=%.2f" % (linear_rmse, linear_r2))
for i, p_value in enumerate(p_values.index):
  print(f'P({p_value}): {p_values[i]}')

if any(p_values <= 0.05):
  print("There is a significant relationship between the predictor and the response\n")
else:
  print("There is no significant relationship between the predictor and the response\n")

# Plot the models
import plotly.graph_objects as go

# Convert timestamp to datetime for x-axis
xtt_before = pd.to_datetime(x_before.squeeze(), unit='s')
xtt_after = pd.to_datetime(x_after.squeeze(), unit='s')

# Create scatter plot traces for actual data
scatter_actual_before = go.Scatter(x=xtt_before, y=y_before, mode='markers', name='Count (Before)', marker=dict(color='blue', opacity=0.5))
scatter_actual_after = go.Scatter(x=xtt_after, y=y_after, mode='markers', name='Count (After)', marker=dict(color='green', opacity=0.5))

# Create line plot traces for linear regression models
line_regression_before = go.Scatter(x=xtt_before, y=y_linear_before, mode='lines', name='LR (Before)', line=dict(color='red'))
line_regression_after = go.Scatter(x=xtt_after, y=y_linear_after, mode='lines', name='LR (After)', line=dict(color='orange'))

# Add vertical line on June 1st
vertical_line = go.Scatter(x=[pd.to_datetime('2020-06-01')]*2, y=[min(y_before.min(), y_after.min()), max(y_before.max(), y_after.max())],
                           mode='lines', name='Tuob/Suob Endorsement', line=dict(color='black'))

# Combine the traces
data = [scatter_actual_before, scatter_actual_after, line_regression_before, line_regression_after, vertical_line]

# Set the layout
layout = go.Layout(
    xaxis=dict(title='Date'),
    yaxis=dict(title='Tweet Count'),
    title='Segmented LR for COVID-19 Mis/Disinformation Tweets',
    showlegend=True,
    height=600
)

# Create the figure
fig = go.Figure(data=data, layout=layout)

# Show the figure
fig.show()</code></pre>
								<p>
									The resulting linear regression for both before and after the endorsement indicate that no significant relationship was found. 
									We do have to note that the number of tweets during this time period is very low. As we can see in the graph, only 1 tweet was 
									found during the 3 month period before the event and only 3 tweets were found during the 3 month period after the event.
								</p>
							</div>
							<div class="container"  style="text-align:left;">
								<h4>T-Test</h4>
								<pre style="tab-size: 4;"><code class="language-python">from scipy.stats import ttest_ind
from scipy.stats import ttest_rel
import numpy as np

# Perform t-test between the slopes
t_statistic, p_value = ttest_ind(slope_before, slope_after, equal_var=False)
# t_statistic, p_value = ttest_rel(slope_before, slope_after)

print("slope_before:", slope_before)
print("slope_after", slope_after)
print("T-test Results")
print("T-statistic:", t_statistic)
print("P-value:", p_value)</code>
								</pre>
								<p>Performing the T-Test, we end up receiving an error message of </p>
								<pre style="tab-size: 4;"><code class="language-md">Precision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.</code>
								</pre>
								<p>
									This error most likely occured due to the overall lack of activity during this period based on the data. 
									Thus with the T-Test failing, there is no conclusive evidence for difference, with our result being not significant 
									due to the lack of data. This lack of data may be due to its non-existence or due to tweet deletion.
								</p>
							</div>
						</div>
						<div class="container" style="text-align:left;">
							<h3>1.b. Salabat</h3>
							<p>Similar to what was done for tuob/suob, we perform segmented linear regression and t-test for the endorsement date for salabat.</p>
							<div class="container"  style="text-align:left;">
								<h4>Segmented Linear Regression</h4>
								<pre style="tab-size: 4;"><code class="language-python">## Perform regression modeling
from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score, mean_squared_error

import statsmodels.api as sm

# Convert datetime to int
x = df_dataset_count['date'].astype(int) / 10**9  # Convert to seconds (UNIX epoch start)
x = x.values.reshape(-1, 1)

y = df_dataset_count['count']

# Divide the dataset based on the date (before and after February 7, 2020)
before_mask = df_dataset_count['date'] < pd.Timestamp('2020-02-07')
after_mask = df_dataset_count['date'] >= pd.Timestamp('2020-02-07')

x_before = x[before_mask][-6:]
y_before = y[before_mask][-6:]

x_after = x[after_mask][:6]
y_after = y[after_mask][:6]</code></pre>
								<pre style="tab-size: 4;"><code class="language-python">#--------------------------------------------------------------
# Linear Regression Model - Before Edorsement
#--------------------------------------------------------------

# Stastical approach
x_lms = sm.add_constant(x_before)
linear_model_stat = sm.OLS(y_before, x_lms)
lms_results = linear_model_stat.fit()
p_values = lms_results.pvalues[1:]

# Calculate predicted values using the statistical slope
y_linear_before = x_lms.dot(lms_results.params)

# Calculate R2 and RMSE for linear regression model
linear_r2 = r2_score(y_before, y_linear_before)
linear_rmse = np.sqrt(mean_squared_error(y_before, y_linear_before))

# Get the slope (coefficient) from the fitted model
slope_before = lms_results.params[1]
print("Slope of the line (Statistical Approach):", slope_before)

print("\nModel Evaluation")
print("Linear Regression: RMSE=%.2f, R2=%.2f" % (linear_rmse, linear_r2))
for i, p_value in enumerate(p_values.index):
	print(f'P({p_value}): {p_values[i]}')

if any(p_values <= 0.05):
	print("There is a significant relationship between the predictor and the response\n")
else:
	print("There is no significant relationship between the predictor and the response\n")

#--------------------------------------------------------------
# Linear Regression Model - After Edorsement
#--------------------------------------------------------------
# Stastical approach
x_lms = sm.add_constant(x_after)
linear_model_stat = sm.OLS(y_after, x_lms)
lms_results = linear_model_stat.fit()
p_values = lms_results.pvalues[1:]

# Calculate predicted values using the statistical slope
y_linear_after = x_lms.dot(lms_results.params)

# Calculate R2 and RMSE for linear regression model
linear_r2 = r2_score(y_after, y_linear_after)
linear_rmse = np.sqrt(mean_squared_error(y_after, y_linear_after))

# Get the slope (coefficient) from the fitted model
slope_after = lms_results.params[1]
print("Slope of the line (Statistical Approach):", slope_after)

print("\nModel Evaluation")
print("Linear Regression: RMSE=%.2f, R2=%.2f" % (linear_rmse, linear_r2))
for i, p_value in enumerate(p_values.index):
	print(f'P({p_value}): {p_values[i]}')

if any(p_values <= 0.05):
	print("There is a significant relationship between the predictor and the response\n")
else:
	print("There is no significant relationship between the predictor and the response\n")

# Plot the models
import plotly.graph_objects as go

# Convert timestamp to datetime for x-axis
xtt_before = pd.to_datetime(x_before.squeeze(), unit='s')
xtt_after = pd.to_datetime(x_after.squeeze(), unit='s')

# Create scatter plot traces for actual data
scatter_actual_before = go.Scatter(x=xtt_before, y=y_before, mode='markers', name='Count (Before)', marker=dict(color='blue', opacity=0.5))
scatter_actual_after = go.Scatter(x=xtt_after, y=y_after, mode='markers', name='Count (After)', marker=dict(color='green', opacity=0.5))

# Create line plot traces for linear regression models
line_regression_before = go.Scatter(x=xtt_before, y=y_linear_before, mode='lines', name='LR (Before)', line=dict(color='red'))
line_regression_after = go.Scatter(x=xtt_after, y=y_linear_after, mode='lines', name='LR (After)', line=dict(color='orange'))

# Add vertical line on June 1st
vertical_line = go.Scatter(x=[pd.to_datetime('2020-02-07')]*2, y=[min(y_before.min(), y_after.min()), max(y_before.max(), y_after.max())],
							mode='lines', name='Salabat Endorsement', line=dict(color='black'))

# Combine the traces
data = [scatter_actual_before, scatter_actual_after, line_regression_before, line_regression_after, vertical_line]

# Set the layout
layout = go.Layout(
	xaxis=dict(title='Date'),
	yaxis=dict(title='Tweet Count'),
	title='Segmented LR for COVID-19 Mis/Disinformation Tweets',
	showlegend=True,
	height=600
)

# Create the figure
fig = go.Figure(data=data, layout=layout)

# Show the figure
fig.show()</code></pre>
							</div>
							<div class="container"  style="text-align:left;">
								<h4>T-Test</h4>
								<pre style="tab-size: 4;"><code class="language-python">from scipy.stats import ttest_ind
from scipy.stats import ttest_rel
import numpy as np

# Perform t-test between the slopes
t_statistic, p_value = ttest_ind(slope_before, slope_after, equal_var=False)
# t_statistic, p_value = ttest_rel(slope_before, slope_after)

print("slope_before:", slope_before)
print("slope_after", slope_after)
print("T-test Results")
print("T-statistic:", t_statistic)
print("P-value:", p_value)</code></pre>
								<p>
									Note that we obtain similar results in testing for salabat as we did for tuob/suob. 
									Note that the time period for before the endosements is limited to three points (1 1/2 months) 
									as January 7 is the earliest tweet that the dataset contains.
								</p>
							</div>
						</div>
					</div>
					<div class="container" id="machine-learning" style="text-align:left; margin-left: 40px;">
						<h3>2. Machine Learning/Computational Modeling</h3>
						<div class="container"  style="text-align:left;">
							<h3>2.a. Visualization</h3>
							<p>
								As our topic focuses on misinformation, the data to be analyzed will be quantititative focused, 
								making the use of event detection the main tool for our machine learning analysis. 
							</p>
							<div class="container"  style="text-align:left;">
								<h4>Visualization of data by week</h4>
								<p>
									To have a good idea on how our data is spread out, we decided to plot our dataset week by week. 
									This allowed us to see how the disinformation regarding our topic behaved over time during the 
									span of the pandemic.</p>
								<pre style="tab-size: 4;"><code class="language-python">## Plot data by day of week
import plotly.graph_objects as go
df_dataset_count['weekday'] = df_dataset_count['date'].dt.strftime('%a')

df_dataset_count['week'] = df_dataset_count['date'].dt.strftime('%Y-W%U')  # Create a 'week' column based on the year and week number
weekly_counts = df_dataset_count.groupby(['week', 'weekday'])['count'].sum().reset_index()

# Sort the weekdays in the correct order
weekday_order = ['Sun', 'Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat']
weekly_counts['weekday'] = pd.Categorical(weekly_counts['weekday'], categories=weekday_order, ordered=True)
weekly_counts = weekly_counts.sort_values(['week', 'weekday'])

fig = go.Figure()

for week, data in weekly_counts.groupby('week'):
	fig.add_trace(go.Scatter(
	x=data['weekday'],
	y=data['count'],
	mode='lines+markers',
	name=week))

fig.update_layout(
	title='Weekly Counts',
	xaxis={'categoryorder': 'array', 'categoryarray': weekday_order},
	yaxis_title='Count')

fig.show()</code></pre>
								<p>
									The graph shows us the distribution of tweets on a given week. We can observe from this graph 
									that there seems to be more activity early in the week, Monday in particular, over other days. 
									We can also pinpoint three weeks with the highest activity in a single day, namely the Monday 
									of ```2021-W17```, Monday of ```2021-W40```, and Wednesday of ```2021-W49```.
								</p>
							</div>
							<div class="container"  style="text-align:left;">
								<h4>Peak Point Detection</h4>
								<p>
									We now turn our attention to idenfitying what else could have caused the influx of disinformation tweets. 
									To accomplish this, we used the peak detection and change point detection models of machine learning.
								</p>
								<p>
									We begin with Peak Detection to identify what dates to search for notable and historical events.
								</p>
								<p>
									In order to perform peak detection, we made use of the scipy module by importing the find_peaks function. 
									This function allowed us to find all local maxima in a given graph by simple comparison of neighboring values.
								</p>
								<pre style="tab-size: 4;"><code class="language-python">## Perform peak detection
from scipy.signal import find_peaks

x = df_dataset_14day['date'].astype(int) / 10**9  # Convert to seconds (UNIX epoch start)
x = x.values.reshape(-1, 1) 
y = df_dataset_14day['count']

peaks, _ = find_peaks(df_dataset_14day['count'],
						height=5,  # height of peaks
						width=1,       # width of peaks
						threshold=1, # vertical distance to its neighboring samples
						distance=7,   # minimal horizontal distance (>= 1) in samples between neighbouring peaks
						prominence=4) # vertical distance between the peak and its lowest contour line


# Extract event timestamps
events = df_dataset_14day.iloc[peaks]

# Plot peaks
fig = go.Figure()

# Original data
fig.add_trace(go.Scatter(
	x=df_dataset_14day.index,
	y=df_dataset_14day['count'],
	hovertext=df_dataset_14day['date'].dt.strftime('%Y-%m-%d'),
	mode='lines',
	name='Original Data'))

# Peaks
fig.add_trace(go.Scatter(
	x=events.index,
	y=events['count'],
	hovertext=events['date'].dt.strftime('%Y-%m-%d'),
	mode='markers',
	name='Peaks',
	marker=dict(
	color='red',
	size=8,
	symbol='x')))

# Additional points
tuob_endorsement_date = '2020-06-01'
salabat_date = '2020-02-07'

tuob_endorsement_index = df_dataset_14day['date'].sub(pd.Timestamp(tuob_endorsement_date)).abs().idxmin()
salabat_index = df_dataset_14day['date'].sub(pd.Timestamp(salabat_date)).abs().idxmin()

fig.add_trace(go.Scatter(
	x=[tuob_endorsement_index for x in range(10)],
	y=[y_1, y_2, y_3, y_4, y_5, y_6, y_7, y_8, y_9, y_10],
	mode='lines',
	name='Tuob Endorsement',
	hoverinfo='text',
	hovertext='Tuob Endorsement<br>%s' % df_dataset_14day.loc[tuob_endorsement_index, 'date'].strftime('%Y-%m-%d'),
	line=dict(color='green', width=1, dash='dash')
))


fig.add_trace(go.Scatter(
	x=[salabat_index for x in range(10)],
	y=[y_1, y_2, y_3, y_4, y_5, y_6, y_7, y_8, y_9, y_10],
	mode='lines',
	name='Salabat Endorsement',
	hoverinfo='text',
	hovertext='Salabat Endorsements<br>%s' % df_dataset_14day.loc[salabat_index, 'date'].strftime('%Y-%m-%d'),
	line=dict(color='blue', width=1, dash='dash')
))

fig.update_layout(height=600,
					title='Twitter COVID Misinformation Peak Events',
					xaxis_title='Time',
					yaxis_title='Number of Tweets')

xtv = np.arange(0,len(df_dataset_14day),10)
xtt = df_dataset_14day['date'].dt.strftime('%Y-%m-%d')[xtv]
fig.update_xaxes(tickmode='array',
					tickvals=xtv,
					ticktext=xtt)

fig.show()</code></pre>
								<p>
									Through peak detection, we identified 4 notable local maxima of peaks in our graph throughout 2020 
									until 2022, with the tweet count hitting a global maximum during around April 2021. Upon further 
									research on the peak dates and events occuring around those times, we found that some of the peaks 
									coincide closely with spikes in total active COVID-19 cases in the Philippines. Here is the 
									comparision between the dataset peaks and COVID-19 active cases peaks.
								</p>
								<p>
									Reference for covid cases: https://doh.gov.ph/covid19tracker
								</p>
								<p><b>Dataset Peaks | COVID-19 Peaks</b></p>
								<ul>
									<li>4/13/21 | 4/14/21 (Very Close)</li>
									<li>8/17/21 | 9/2/21 (Close)</li>
									<li>1/18/21 | 1/12/21 (Very Close)</li>
									<li>11/22/22 | 8/10/22 (Far)</li>
								</ul>
								<p>
									Two out of the four peaks are within a week's distance, and another one is within a month's distance. 
									Only the most recent peak, which is also the smallest peak, is significantly off with a distance of 
									around 3 months.
								</p>
							</div>
							<div class="container"  style="text-align:left;">
								<h4>Change Point Detection</h4>
								<p>
									Following this, our group made use of the ruptures module in order to perform Change Point Detection 
									in order to identify the points in the dataset where trends in our graph start or finish.
								</p>
								<pre style="tab-size: 4;"><code class="language-python">import ruptures as rpt
import plotly.graph_objects as go
import numpy as np

data = df_dataset_14day['count'].values

# Change-point detection via Pelt algorithm
model = "rbf"
algo = rpt.Pelt(model=model).fit(data)
result = algo.predict(pen=.9)  # Adjust the penalty value based on your data

fig = go.Figure()

# Original data
fig.add_trace(go.Scatter(
	x=df_dataset_14day.index,
	y=df_dataset_14day['count'],
	hovertext=df_dataset_14day['date'].dt.strftime('%Y-%m-%d'),
	mode='lines',
	name='Original Data'))

# Change points
for cp_index in result[:-1]:
	x_1 = df_dataset_14day.index[cp_index]
	y_1, y_2, y_3, y_4, y_5 = -2, 1, 4, 7, 10
	y_6, y_7, y_8, y_9, y_10 = 13, 16, 19, 22, 25

	fig.add_trace(go.Scatter(
	x=[x_1, x_1, x_1, x_1, x_1, x_1, x_1, x_1, x_1, x_1],
	y=[y_1, y_2, y_3, y_4, y_5, y_6, y_7, y_8, y_9, y_10],
	mode='lines',
	name=df_dataset_14day['date'].dt.strftime('%Y-%m-%d')[cp_index],
	hoverinfo='text',
	hovertext='Change at<br>%s' % df_dataset_14day['date'].dt.strftime('%Y-%m-%d')[cp_index],
	line=dict(color='red', width=1, dash='dash')))

# Additional points
tuob_endorsement_date = '2020-06-01'
salabat_date = '2020-02-07'

tuob_endorsement_index = df_dataset_14day['date'].sub(pd.Timestamp(tuob_endorsement_date)).abs().idxmin()
salabat_index = df_dataset_14day['date'].sub(pd.Timestamp(salabat_date)).abs().idxmin()

fig.add_trace(go.Scatter(
	x=[tuob_endorsement_index for x in range(10)],
	y=[y_1, y_2, y_3, y_4, y_5, y_6, y_7, y_8, y_9, y_10],
	mode='lines',
	name='Tuob Endorsement',
	hoverinfo='text',
	hovertext='Tuob Endorsement<br>%s' % df_dataset_14day.loc[tuob_endorsement_index, 'date'].strftime('%Y-%m-%d'),
	line=dict(color='green', width=1, dash='dash')
))


fig.add_trace(go.Scatter(
	x=[salabat_index for x in range(10)],
	y=[y_1, y_2, y_3, y_4, y_5, y_6, y_7, y_8, y_9, y_10],
	mode='lines',
	name='Salabat Endorsement',
	hoverinfo='text',
	hovertext='Salabat Endorsements<br>%s' % df_dataset_14day.loc[salabat_index, 'date'].strftime('%Y-%m-%d'),
	line=dict(color='blue', width=1, dash='dash')
))

fig.update_layout(height=600,
					title='Twitter COVID Misinformation Change Points',
					xaxis_title='Time',
					yaxis_title='Number of Tweets')

xtv = np.arange(0, len(df_dataset_14day), 10)
xtt = df_dataset_14day['date'].dt.strftime('%Y-%m-%d')[xtv]
fig.update_xaxes(tickmode='array',
					tickvals=xtv,
					ticktext=xtt)

fig.show()</code></pre>
								<p>
									Using the penalty value of 0.9, we were able to find a total of 6 change points in our dataset.
								</p>
								<p>
									With 4 change points coinciding with upticks of Covid cases in the country:
								</p>
								<ul>
									<li>March 2, 2021</li>
									<li>July 20, 2021</li>
									<li>September 28, 2021</li>
									<li>September 13, 2022</li>
								</ul>
								<p>
									With the remaining 2 change points coinciding with Covid cases either plateauing or falling off:
								</p>
								<ul>
									<li>May 11, 2021</li>
									<li>April 26, 2022</li>
								</ul>
							</div>
						</div>
					</div>
					<footer>
						<p>Want to learn more? Take a look at our source codes.</p>
						<a href="https://github.com/jltsang/COVID-Alternative-Cures-Misinformation" class="button large scrolly" target="_blank">Github Repo</a>
					</footer>
				</div>
			</article>

			<!-- Results -->
			<!-- <article id="results" class="wrapper style1">
				<div class="container">
					<header>
						<h2>Here's what we found out.</h2>
						<p>Proin odio consequat  sapien vestibulum consequat lorem dolore feugiat.</p>
					</header>
					<div class="row">
						<div class="col-6 col-6-medium col-12-small">
							<article class="box style2">
								<a href="#" class="image featured"><img src="images/pic02.jpg" alt="" /></a>
								<h3><a href="#">Results</a></h3>
								<p>Ornare nulla proin odio consequat.</p>
							</article>
						</div>
						<div class="col-6 col-6-medium col-12-small">
							<article class="box style2">
								<a href="#" class="image featured"><img src="images/pic01.jpg" alt="" /></a>
								<h3><a href="#">Results</a></h3>
								<p>Ornare nulla proin odio consequat.</p>
							</article>
						</div>
					</div>
					<footer>
						<p>Want to learn more? Take a look at our source codes.</p>
						<a href="https://github.com/jltsang/COVID-Alternative-Cures-Misinformation" class="button large scrolly" target="_blank">Github Repo</a>
					</footer>
				</div>
			</article> -->

		<!-- Team -->
			<article id="team" class="wrapper style2">
				<div class="container medium">
					<header>
						<h2>Team Members</h2>
					</header>
					<section class="box style1">
						<div class="column">
							<div class="col-4 col-6-medium col-12-small">
								<h3>Francis M. Albarracin</h3>
								<a href="mailto:fmalbarracin@up.edu.ph">fmalbarracin@up.edu.ph</a>
							</div>
							<div class="col-8 col-6-medium col-12-small">
								<p>Hello! I’m Francis M. Albarracin, currently a 3rd year BS Computer Science student at the University of the Philippines - Diliman. I’ve always been interested in technology ever since I was a child and am always looking to learn more about all the different technologies and gadgets that we use today. Some of my hobbies include learning Japanese, basketball, incremental improvement, and optimization 🙂.</p>
							</div>	
						</div>
					</section>
					<section class="box style1">
						<div class="column">
							<div class="col-4 col-6-medium col-12-small">
								<h3>Zachary R. Nabong</h3>
								<a href="mailto:zrnabong@up.edu.ph">zrnabong@up.edu.ph</a>
							</div>
							<div class="col-8 col-6-medium col-12-small">
								<p>Hello! I’m Zachary R. Nabong, and I am a 2nd year BS Computer Science student at University of the Philippines - Diliman. Coding has always been something that has interested me, more specifically game development and data science. My hobbies include playing video games such as CSGO, Valorant, Rainbow Six and Dota 2.</p>
							</div>	
						</div>
					</section>

					<section class="box style1">
						<div class="column">
							<div class="col-4 col-6-medium col-12-small">
								<h3>Jeremy King L. Tsang</h3>
								<a href="mailto:jltsang1@up.edu.ph">jltsang1@up.edu.ph</a>
							</div>
							<div class="col-8 col-6-medium col-12-small">
								<p>Hi! I’m Jeremy King L. Tsang, a 4th year BS Computer Science student at University of the Philippines - Diliman. I am currently an undergraduate researcher at Service Science and Software Engineering Laboratory (S3), where I am focusing on identifying and improving student recruitment systems of higher education institutions in the Philippines. In my free time, I am fond of reading light novels online and practicing to play the piano. </p>
							</div>	
						</div>
					</section>
					<header>
						<h2>We'd like to hear from you.</h2>
						<p>Please fill out this <a href="https://forms.gle/e5AFmiH5HtX4tKSx8" target="_blank">form</a> to send us your comments and suggestions.</p>
					</header>
					<!-- <div class="row">
						<div class="col-12">
							<form method="post" action="#">
								<div class="row">
									<div class="col-6 col-12-small">
										<input type="text" name="name" id="name" placeholder="Name" />
									</div>
									<div class="col-6 col-12-small">
										<input type="text" name="email" id="email" placeholder="Email" />
									</div>
									<div class="col-12">
										<input type="text" name="subject" id="subject" placeholder="Subject" />
									</div>
									<div class="col-12">
										<textarea name="message" id="message" placeholder="Message"></textarea>
									</div>
									<div class="col-12">
										<ul class="actions">
											<li><input type="submit" value="Send Message" /></li>
											<li><input type="reset" value="Clear Form" class="alt" /></li>
										</ul>
									</div>
								</div>
							</form>
						</div> -->
						<!-- <div class="col-12">
							<hr />
							<h3>Find me on ...</h3>
							<ul class="social">
								<li><a href="#" class="icon brands fa-twitter"><span class="label">Twitter</span></a></li>
								<li><a href="#" class="icon brands fa-facebook-f"><span class="label">Facebook</span></a></li>
								<li><a href="#" class="icon brands fa-dribbble"><span class="label">Dribbble</span></a></li>
								<li><a href="#" class="icon brands fa-linkedin-in"><span class="label">LinkedIn</span></a></li>
								<li><a href="#" class="icon brands fa-tumblr"><span class="label">Tumblr</span></a></li>
								<li><a href="#" class="icon brands fa-google-plus"><span class="label">Google+</span></a></li>
								<li><a href="" class="icon brands fa-github"><span class="label">Github</span></a></li>
								
								<li><a href="#" class="icon solid fa-rss"><span>RSS</span></a></li>
								<li><a href="#" class="icon brands fa-instagram"><span>Instagram</span></a></li>
								<li><a href="#" class="icon brands fa-foursquare"><span>Foursquare</span></a></li>
								<li><a href="#" class="icon brands fa-skype"><span>Skype</span></a></li>
								<li><a href="#" class="icon brands fa-soundcloud"><span>Soundcloud</span></a></li>
								<li><a href="#" class="icon brands fa-youtube"><span>YouTube</span></a></li>
								<li><a href="#" class="icon brands fa-blogger"><span>Blogger</span></a></li>
								<li><a href="#" class="icon brands fa-flickr"><span>Flickr</span></a></li>
								<li><a href="#" class="icon brands fa-vimeo"><span>Vimeo</span></a></li>
								
							</ul>
							<hr />
						</div> -->
					</div>
					<footer>
						<ul id="copyright">
							<li>&copy; Untitled. All rights reserved.</li><li>Design: <a href="http://html5up.net">HTML5 UP</a></li>
						</ul>
					</footer>
				</div>
			</article>

		<!-- Scripts -->
			<script src="assets/js/jquery.min.js"></script>
			<script src="assets/js/jquery.scrolly.min.js"></script>
			<script src="assets/js/browser.min.js"></script>
			<script src="assets/js/breakpoints.min.js"></script>
			<script src="assets/js/util.js"></script>
			<script src="assets/js/main.js"></script>
			<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/11.7.0/highlight.min.js"></script>
			<script>
				hljs.highlightAll();
			</script>
	</body>
</html>
