<!DOCTYPE HTML>
<!--
	Miniport by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>
	<head>
		<title>PH Twitter Fake News Analysis</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="assets/css/main.css" />
		<link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/11.7.0/styles/hybrid.min.css">
	</head>
	<body class="is-preload">

		<!-- Nav -->
		<nav id="nav">
			<ul class="container">
				<li><a href="./index.html#top">Top</a></li>
				<li><a href="./index.html#overview">Overview</a></li>
				<li><a href="./index.html#data">Data</a></li>
				<li><a href="./index.html#methods">Methods</a></li>
				<li><a href="./index.html#results">Results</a></li>
				<li><a href="./index.html#team">Team</a></li>
			</ul>
		</nav>

		<!-- Data exploration -->
		<article id="methods" class="wrapper style2">
			<div class="container">
				<header>
					<h2>First, we have to clean the data,</h2>
				</header>
				<p>
					Before we begin exploring the data, preprocessing techniques must first be applied 
					in order to ensure the cleanliness of the data.
				</p>
				<div class="container" style="text-align:left;">
					<h3>1. Handling missing values</h3>
					<p>When looking for missing values, we can observe that <span class="emph">Account bio</span>, `Location`, `Tweet Translated`, `Screenshot`, `Views`, `Rating`, `Remarks`, `Reviewer`, `Review` are all listed.</p>
					<pre style="tab-size: 4;" class="prettyprint">
						<code class="language-python">
							print("Columns with missing/nan values:")
							print("Data set:", df_dataset.columns[df_dataset.isna().any()].tolist())
						</code>
					</pre>
					<figure>
						<img src="images/1a-output1.png" alt="" srcset="" style="width: 100%">
						<figcaption>Codeblock output</figcaption>
					</figure>
					<p>Some of these columns, including `Tweet Translated`, `Screenshot`, `Views`, `Rating`, `Remarks`, `Reviewer`, and `Review`, are optional columns that we have opted not to fill out during data collection.</p>
					<p>On the other hand, `Account bio` and `Location` are columns with missing values given that Twitter users may opt to leave these fields blank. Given that there is already a limited amount of data, dropping the samples is not ideal. In addition, `Account bio` and `Location` are not relevant variables for what we want to analyse later on. Therefore, given the irrelevancy of said columns and small number of samples, we instead choose to drop the columns instead of the samples.</p>
					<pre style="tab-size: 4;">
						<code class="language-python">df_dataset = df_dataset.drop(['Account bio', 'Location', 'Tweet Translated', 'Screenshot', 'Rating', 'Remarks','Reviewer', 'Review', 'Views'], axis=1)	</code>
					</pre>
					<p>In addition to columns with missing values, we can also look to drop columns that are completely filled up but are irrelevant to analysis. These include `ID`, `Timestamp`, `Tweet URL`, `Group`, `Collector`, `Category`, `Topic`, `Keywords`, `Account handle`, `Account name`, `Joined`, `Tweet Type`, and `Reasoning`.</p>
					<pre style="tab-size: 4;">
						<code class="language-python">df_dataset = df_dataset.drop(['ID', 'Timestamp', 'Group', 'Collector', 'Category', 'Topic', 'Keywords', 'Reasoning', 'Tweet URL', 'Account handle', 'Account name', 'Joined', 'Tweet Type'], axis=1)	</code>
					</pre>
					<p>With that, we are left with a much more compact dataset with only 10 columns, while maintaining a total of 150 samples.</p>
					<pre style="tab-size: 4;">
						<code class="language-python">df_dataset.head(10).style.set_properties(**{'text-align': 'left'})	</code>
					</pre>
					<figure>
						<img src="images/1a-output2.png" alt="" srcset="" style="width: 100%">
						<figcaption>The first 10 rows of the dataframe</figcaption>
					</figure>
				</div>
				<div class="container" style="text-align:left;">
					<h3>2. Handling outliers</h3>
					<p>After handling missing values, we turn our attention into possible outlier values. While we were able to preserve our samples by dropping columns when handling missing values, the same cannot be done for outlier data. These data can heavily affect our analysis and results later on.</p>
					<p>Using the following code, we begin by looking into our outlier data based on each variable.</p>
					<pre style="tab-size: 4;">
						<code class="language-python">
							from sklearn.preprocessing import StandardScaler
							import seaborn as sns

							# Standardize the features to get z-scores
							df_scaled = df_dataset.copy(deep=True)
							scaler = StandardScaler()
							df_scaled[['Followers']] = scaler.fit_transform(df_scaled[['Followers']])

							followers_zscore = df_scaled['Followers']
							n_out1 = len(followers_zscore[abs(followers_zscore) > 1])
							n_out2 = len(followers_zscore[abs(followers_zscore) > 2])
							n_out3 = len(followers_zscore[abs(followers_zscore) > 3])
							print(f"Number of outliers in 'Followers' (std=1): {n_out1}")
							print(f"Number of outliers in 'Followers' (std=2): {n_out2}")
							print(f"Number of outliers in 'Followers' (std=3): {n_out3}")

							# Define function for coloring bars
							def color_bars(ax, df):
								for i in range(len(df)):
									val = abs(df.loc[i, 'Followers'])
									if val <= 1:
										color = 'b'
									elif val <= 2:
										color = 'g'
									elif val <= 3:
										color = 'y'
									else:
										color = 'r'
									ax.get_children()[i].set_color(color)

							# Create a figure with two subplots
							fig, (ax1, ax2) = plt.subplots(nrows=2, figsize=(16, 12))

							# Plot the Followers distribution
							sns.barplot(x=df_scaled.index, y=df_scaled['Followers'], ax=ax1)

							sns.despine(ax=ax1)
							ax1.set(title='Followers Outliers', xlabel='Sample', ylabel='Followers (z-score)')
							ax1.set_xticks([0, len(df_scaled)-1])
							ax1.axhline(y=3, xmin=-0.5, xmax=len(df_scaled)-0.5, color='red', alpha=0.5, ls='--') # Standard deviation lines
							ax1.axhline(y=-3, xmin=-0.5, xmax=len(df_scaled)-0.5, color='red', alpha=0.5, ls='--')
							color_bars(ax1, df_scaled) # Color the bars

							# Plot the Followers distribution, but sorted
							df_scaled_sorted = df_scaled.sort_values(['Followers']).reset_index(drop=True)
							sns.barplot(x=df_scaled_sorted.index, y=df_scaled_sorted['Followers'], ax=ax2)

							sns.despine(ax=ax2)
							ax2.set(title='Followers Outliers', xlabel='Sample (sorted)', ylabel='Followers (z-score)')
							ax2.set_xticks([0, len(df_scaled_sorted)-1])
							ax2.axhline(y=3, xmin=-0.5, xmax=len(df_scaled_sorted)-0.5, color='red', alpha=0.5, ls='--') # Standard deviation lines
							ax2.axhline(y=-3, xmin=-0.5, xmax=len(df_scaled_sorted)-0.5, color='red', alpha=0.5, ls='--')
							color_bars(ax2, df_scaled_sorted) # Color the bars

							plt.tight_layout()
							plt.show()
						</code>
					</pre>
					<figure>
						<img src="images/1b-output1.png" alt="" style="width: 100%">
						<img src="images/1b-output2.png" alt="" style="width: 100%">
						<img src="images/1b-output3.png" alt="" style="width: 100%">
						<img src="images/1b-output4.png" alt="" style="width: 100%">
						<figcaption>Followers, Likes, Replies, and Retweets distributions</figcaption>
					</figure>
					<p>We can observe that outlier data do exist in our dataset. Even more concerning is the fact that these outlier data go as high 10 standard deviations away (std=10). In order to make sure we that these samples would not skew our analysis later, we can only drop them.</p>
					<pre style="tab-size: 4;">
						<code class="language-python">
							def remove_outliers(df, feature, threshold):
								df_scaled = df.copy(deep=True)
								scaler = StandardScaler()
								df_scaled[[feature]] = scaler.fit_transform(df_scaled[[feature]])
								mask = abs(df_scaled[feature]) <= threshold
								return df[mask].reset_index(drop=True)

							threshold = 3
							for col in df_dataset.select_dtypes(include='number').columns:
								df_dataset = remove_outliers(df_dataset, col, threshold)
						</code>
					</pre>
				</div>
				<div class="container" style="text-align:left;">
					<h3>3. Ensuring format consistency</h3>
					<p>In order to ensure the validity and consistency of the data, a quick set of tests are conducted to find incorrectly-formatted data. To accomplish this, we created a mask with the correct format for each column and compared it with the dataset entries. As seen below we can see that the results show no formatting mistakes for all columns.</p>
					<pre style="tab-size: 4;">
						<code class="language-python">
							import re

								print("Checking # of incorrectly-formatted rows...")
								# Check account type
								account_type_mask = df_dataset['Account type'].isin(['Anonymous','Identified','Media'])
								print(f"For column \"Account type\": {df_dataset[~account_type_mask].shape[0]}")

								# Check date posted (DD/MM/YY HH:MM)
								date_posted_pattern = re.compile(r'(0[1-9]|[1-2][0-9]|3[0-1])\/(0[1-9]|1[0-2])\/([0-9]{2}) (2[0-3]|[01]?[0-9]):[0-5][0-9]')
								date_posted_mask = df_dataset['Date posted'].astype(str).str.match(date_posted_pattern)
								print(f"For column \"Date posted\": {df_dataset[~date_posted_mask].shape[0]}")

								# Check content type
								content_type_mask = df_dataset['Content type'].isin(['Rational','Emotional','Transactional'])
								print(f"For column \"Content type\": {df_dataset[~content_type_mask].shape[0]}")

								# Check for valid whole numbers
								whole_number_pattern = re.compile(r'^\d+$')
								for col in ['Following', 'Followers', 'Likes', 'Replies', 'Retweets', 'Quote Tweets']:
									number_mask = df_dataset[col].astype(str).str.match(whole_number_pattern)
									print(f"For column \"{col}\": {df_dataset[~number_mask].shape[0]}")
						</code>
					</pre>
					<figure style="text-align: center;">
						<img src="images/1c-output1.png" alt="" style="max-width: 100%;">
						<figcaption>Output of the codeblock</figcaption>
					</figure>
				</div>
				<div class="container" style="text-align:left;">
					<h3>4. Categorical data encoding</h3>
					<p>Looking at our remaining columns, we can observe that there are two categorical variables, `Account type` and `Content type`. In order to use the following variable later on, we will need to convert these columns to numerical data. To accomplish this, we perform one-hot encoding on both categories.</p>
					<pre style="tab-size: 4;">
						<code class="language-python">
							# One-hot encode Account Type column
							account_type_dummies = pd.get_dummies(df_dataset['Account type'], prefix='Account Type')
							df_dataset = pd.concat([df_dataset, account_type_dummies], axis=1)

							# One-hot encode Content Type column
							content_type_dummies = pd.get_dummies(df_dataset['Content type'], prefix='Content Type')
							df_dataset = pd.concat([df_dataset, content_type_dummies], axis=1)

							print(df_dataset.columns)
						</code>
					</pre>
					<figure style="text-align: center;">
						<img src="images/1d-output1.png" alt=""  style="max-width: 100%;">
						<figcaption>Columns of the dataframe</figcaption>
					</figure>
					<p>This will make it easier to perform analysis and modeling with these variables later on, espcially beside other numerical data.</p>
				</div>
				<div class="container" style="text-align:left;">
					<h3>5. Natural Language Processing</h3>
					<p>Now that we have made sure that our data set has no blemishes, we can begin with the processing. We make use of the NLP (Natural Language Processing) module found within Python in order for us to turn our dataset to a more structured and workable set</p>
					<p>We begin our process by 'cleaning' the dataset and transforming special symbols such as emojis into words, as the usage of emojis may provide extra context to the tweets they are being used in.</p>
					<pre style="tab-size: 4;">
						<code class="language-python">
							import pandas as pd
							import re
							import copy
							import nltk

							# Handle Emojis [2]
							url_emoji = "https://drive.google.com/uc?id=1G1vIkkbqPBYPKHcQ8qy0G2zkoab2Qv4v"
							df_emoji = pd.read_pickle(url_emoji)
							df_emoji = {v: k for k, v in df_emoji.items()}

							def emoji_to_word(text):
								for emot in df_emoji:
									text = re.sub(r'('+emot+')', "_".join(df_emoji[emot].replace(",","").replace(":","").split()), text)
								return text

							# Handle Emoticons [2]
							url_emote = "https://drive.google.com/uc?id=1HDpafp97gCl9xZTQWMgP2kKK_NuhENlE"
							df_emote = pd.read_pickle(url_emote)

							def emote_to_word(text):
								for emot in df_emote:
									text = re.sub(u'('+emot+')', "_".join(df_emote[emot].replace(",","").split()), text)
									text = text.replace("<3", "heart" ) # not included in emoticons database
								return text

							texts = copy.deepcopy(df_dataset['Tweet'])

							texts = [emoji_to_word(t) for t in texts]
							texts = [emote_to_word(t) for t in texts]
						</code>
					</pre>
					<p>after converting the emojis, we made use of a new array called `texts` which we used to store the new tweets as we wish to turn all the words into lowercase in order to make the dataset easier to work with.</p>
					<pre style="tab-size: 4;">
						<code class="language-python">
							import string

							# convert to lowercase
							texts = [t.lower() for t in texts]

							# replace tuob and suob with steam
							texts = [t.replace('tuob', 'steam').replace('suob', 'steam') for t in texts]

							# remove punctuation
							texts = [t.translate(str.maketrans('', '', string.punctuation)) for t in texts]
						</code>
					</pre>
					<p>You may have noticed that we included a line of code that replaced `tuob` or `suob` with `steam`. This was done by us as a sort of bandaid solution as the translator was either ignoring the word entirely or misreading the word as `subo` leading to incorrect translation.</p>
					<p>In order to maximize the use of the NLP module, we decided to translate the tweets first from Tagalog to English as the module was not optimized to handle tweets in Tagalog.</p>
					<pre style="tab-size: 4;">
						<code class="language-python">
							# Removing stopwords might be tedious for multilingual texts
							from nltk.corpus import stopwords
							from nltk.tokenize import word_tokenize

							# CHEAP SOLUTION: translate texts to English (this is not 100% accurate)
							from googletrans import Translator

							# translate to English
							translator = Translator()
							texts_en = [t.text for t in translator.translate(texts, src='tl', dest='en')]
						</code>
					</pre>
					<p>Now that we converted the dataset into English, we could know start with actually structuring the data. First we started by tokenizing the data. Tokenizing is when we conviently split up text word by word. This allowed us to work with smaller pieces of data that were still meaningful even outside of context of the rest of the text. </p>
					<p>After tokenizing, we also made sure to remove the stopwords in our dataset. Stopwords are pretty much the words we don't really want to care about, so we filter them out of our data frame.</p>
					<pre style="tab-size: 4;">
						<code class="language-python">
							texts_tok = []
							for text in texts_en:
								# tokenize the text into words
								words = word_tokenize(text)

								# remove stopwords
								filtered_words = [word for word in words if word.lower() not in stopwords.words('english')]

								# convert back into sentence
								filtered_sentence = ' '.join(filtered_words)
								texts_tok.append(filtered_sentence)
						</code>
					</pre>
					<p>The process of tokenizing and stopwords removal was similar to our earlier methods, as you can see with the use of a new array called `texts_tok`.</p>
					<p>With the data now easier to work with, we simplified it further with Stemming and Lemmatization. Stemming is when you strip a word down to its root word. Kind of like turning a fraction into its simplest form. Lemmatization is like stemming but will give you a complete word that makes sense when you read it instead of a fragment of a word.</p>
					<p>We accomplished stemming and lemmatization by making use of separate variables called `stemmer` and `lemmatizer` respectively, which was used in the `stem_lem` function for the process of stemming/lemmatization. </p>
					<pre style="tab-size: 4;">
						<code class="language-python">
							from nltk.stem import PorterStemmer, WordNetLemmatizer

							# Initialize the stemmer and lemmatizer
							stemmer = PorterStemmer()
							lemmatizer = WordNetLemmatizer()

							texts_stem, texts_lem = [], []

							def stem_lem(text):
								words = text.split()

								# Stem each word
								stemmed_words = [stemmer.stem(word) for word in words]
								
								# Lemmatize each word
								lemmatized_words = [lemmatizer.lemmatize(word) for word in words]
								
								# Return the stemmed and lemmatized words as a tuple
								texts_stem.append(stemmed_words)
								texts_lem.append(lemmatized_words)

								return (stemmed_words, lemmatized_words)


							# Process each text in the array
							processed_texts = [stem_lem(t) for t in texts_tok]


							df_sl = pd.DataFrame({'Original': df_filt['Tokenized'], 'Stemmed': texts_stem, 'Lemmatized': texts_lem})
							df_sl.style.set_properties(**{'text-align': 'left'})
						</code>
					</pre>
					<figure>
						<img src="images/1e-output1.png" alt="" style="width:100%;">
						<figcaption>First 10 rows of the new dataframes</figcaption>
					</figure>
					<p>The final result from all the processing was the creation of a final dataframe called `df_sl` and this dataframe was used in getting the common `stemmed` and `lemmatized` words that gave us an insight as to which "cure" was most prevalent among the tweets.</p>
					<pre style="tab-size: 4;">
						<code class="language-python">
							stemm_freq = df_sl['Stemmed'].explode().value_counts()
							stemm_freq[:30].to_frame().style
						</code>
					</pre>
					<figure style="text-align: center;">
						<img src="images/1e-output2.png" alt="">
						<figcaption>Most common words from the Stemmed dataframe</figcaption>
					</figure>
					<pre style="tab-size: 4;">
						<code class="language-python">
							lemm_freq = df_sl['Lemmatized'].explode().value_counts()
							lemm_freq[:30].to_frame().style
						</code>
					</pre>
					<figure style="text-align: center;">
						<img src="images/1e-output3.png" alt="">
						<figcaption>Most common words from the Lemmatized dataframe</figcaption>
					</figure>
					<p>From the results, it was concluded that `steam` in this case `tuob` or `suob` was the most talked about "cure" for COVID, followed by `ginger`, `lemon`, `salt` and lastly, `vitamins`.</p>
				</div>
				<div class="container" style="text-align:left;">
					<h3>6. Time series analysis binning</h3>
					<p>
						In order to do a time series analysis of our data, specifically the `Date posted` 
						column, there is the option to either interpolate or bin the time points. However 
						given the large range of time in which the data points are collected (2020-2022) 
						and the small size of the dataset, binning is a much more feasible method. 14-day 
						binning and monthly binning were chosen as the intervals. Given the limited dataset, 
						monthly binning has a higher change of providing a clear distinction of spikes and 
						peaks, while 14-day binning would make identifying which specific events (if there 
						are any) may have resulted in the spike/peak. The process of binning can be found 
						with the visualization of the time series thorough line plots.</p>
				</div>
				<footer>
					<a href="./index.html#data" class="button large scrolly">Back to home page</a>
				</footer>
			</div>
			<footer>
				<ul id="copyright">
					<li>&copy; Untitled. All rights reserved.</li><li>Design: <a href="http://html5up.net">HTML5 UP</a></li>
				</ul>
			</footer>
		</article>

		<!-- Scripts -->
		<script src="assets/js/jquery.min.js"></script>
		<script src="assets/js/jquery.scrolly.min.js"></script>
		<script src="assets/js/browser.min.js"></script>
		<script src="assets/js/breakpoints.min.js"></script>
		<script src="assets/js/util.js"></script>
		<script src="assets/js/main.js"></script>
		<script src="assets/js/formatting.js"></script>
		<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/11.7.0/highlight.min.js"></script>
		<script>
			hljs.highlightAll();
		</script>
	</body>
</html>
